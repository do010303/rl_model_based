# ğŸ—ï¸ Project Structure

This document describes the organized structure of the RL Model-Based project.

## ğŸ“ Directory Structure

```
rl_model_based/
â”œâ”€â”€ ğŸ“š docs/                           # Documentation
â”‚   â”œâ”€â”€ README.md                      # Main project documentation
â”‚   â”œâ”€â”€ CONTRIBUTING.md                # Contribution guidelines
â”‚   â”œâ”€â”€ LICENSE                        # License information
â”‚   â”œâ”€â”€ REPORT_09_10_REINFORCEMENT_LEARNING.md
â”‚   â””â”€â”€ TRAINING_IMPROVEMENTS.md       # Training methodology improvements
â”‚
â”œâ”€â”€ ğŸ§  agents/                         # RL Agents
â”‚   â”œâ”€â”€ base_agent.py                  # Base agent interface
â”‚   â””â”€â”€ ddpg_agent.py                  # DDPG implementation
â”‚
â”œâ”€â”€ ğŸŒ environments/                   # Training Environments
â”‚   â””â”€â”€ robot_4dof_env.py              # 4-DOF robot arm environment
â”‚
â”œâ”€â”€ ğŸ“ examples/                       # Training Scripts
â”‚   â”œâ”€â”€ train_ddpg.py                  # Main training script (DDPG/MBPO)
â”‚   â”œâ”€â”€ train_curriculum.py            # Curriculum learning
â”‚   â”œâ”€â”€ test_model.py                  # Model testing utilities
â”‚   â””â”€â”€ visualize_robot.py             # Visualization tools
â”‚
â”œâ”€â”€ ğŸ¤– models/                         # Neural Network Models
â”‚   â””â”€â”€ dynamics_model.py              # World dynamics model
â”‚
â”œâ”€â”€ ğŸ’¾ replay_memory/                  # Experience Replay
â”‚   â””â”€â”€ replay_buffer.py               # Smart cleanup replay buffer
â”‚
â”œâ”€â”€ ğŸ¯ training/                       # Training Utilities
â”‚   â”œâ”€â”€ curriculum.py                  # Curriculum learning logic
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ› ï¸ utils/                          # Utilities
â”‚   â”œâ”€â”€ her.py                         # Hindsight Experience Replay
â”‚   â””â”€â”€ early_stopping.py             # Early stopping utilities
â”‚
â”œâ”€â”€ ğŸ’¾ checkpoints/                    # Model Checkpoints (Organized)
â”‚   â”œâ”€â”€ ddpg/                          # DDPG model checkpoints
â”‚   â”‚   â”œâ”€â”€ ddpg_4dof_actor.h5
â”‚   â”‚   â”œâ”€â”€ ddpg_4dof_critic.h5
â”‚   â”‚   â””â”€â”€ ddpg_4dof_config.json
â”‚   â”œâ”€â”€ mbpo/                          # MBPO model checkpoints
â”‚   â”‚   â”œâ”€â”€ mbpo_4dof_actor.h5
â”‚   â”‚   â”œâ”€â”€ mbpo_4dof_critic.h5
â”‚   â”‚   â””â”€â”€ mbpo_4dof_config.json
â”‚   â”œâ”€â”€ curriculum/                    # Curriculum learning checkpoints
â”‚   â”‚   â””â”€â”€ curriculum_*.h5
â”‚   â””â”€â”€ replay_buffers/                # Saved replay buffers
â”‚       â”œâ”€â”€ replay_buffer.pkl          # DDPG replay buffer
â”‚       â””â”€â”€ mbpo_replay_buffer.pkl     # MBPO replay buffer
â”‚
â”œâ”€â”€ ğŸ“Š logs/                          # Training Logs & Results
â”‚   â”œâ”€â”€ training/                      # Training logs
â”‚   â””â”€â”€ results/                       # Result plots and metrics
â”‚       â””â”€â”€ mbpo_training_results.png
â”‚
â”œâ”€â”€ ğŸ”§ configs/                       # Configuration Files
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â””â”€â”€ setup.sh                      # Environment setup script
â”‚
â”œâ”€â”€ ğŸ“œ scripts/                       # Utility Scripts
â”‚   â”œâ”€â”€ demo.py                       # Demo script
â”‚   â””â”€â”€ test_nan_prevention.py        # Testing utilities
â”‚
â”œâ”€â”€ ğŸ¤– src/                           # ROS Integration (Future Gazebo)
â”‚   â””â”€â”€ New_robot_arm_urdf/           # Robot URDF files for Gazebo
â”‚
â””â”€â”€ ğŸ“„ Core Files
    â”œâ”€â”€ mbpo_trainer.py               # MBPO trainer implementation
    â””â”€â”€ __init__.py                   # Package initialization
```

## ğŸš€ Quick Start

### Training with MBPO (Recommended)
```bash
python3 examples/train_ddpg.py --episodes 300 --method mbpo
```

### Training with DDPG
```bash
python3 examples/train_ddpg.py --episodes 300 --method ddpg
```

## ğŸ“‹ Key Features

### âœ¨ Smart Buffer Management
- **Success-prioritized cleanup** preserves high-reward experiences
- **Competitive buffer cleanup** maintains training quality
- **Dynamic capacity management** prevents memory overflow

### ğŸ§  Advanced RL Algorithms
- **MBPO** (Model-Based Policy Optimization) with dynamics model
- **DDPG** (Deep Deterministic Policy Gradient) 
- **Curriculum Learning** support
- **HER** (Hindsight Experience Replay) integration

### ğŸ¯ Performance Optimizations
- **NaN prevention** in dynamics models
- **Gradient clipping** for stability
- **Batch normalization** for faster convergence
- **Success rate tracking** and early convergence detection

## ğŸ“Š Results Location

All training results are saved in organized locations:
- **Model checkpoints**: `checkpoints/{algorithm}/`
- **Replay buffers**: `checkpoints/replay_buffers/`
- **Training plots**: `logs/results/`
- **Training logs**: `logs/training/`

## ğŸ”§ Configuration

Project configuration files are located in:
- `configs/requirements.txt` - Python dependencies
- `configs/setup.sh` - Environment setup

## ğŸ“š Documentation

All documentation is organized in the `docs/` folder:
- Main README with usage instructions
- Training methodology improvements
- Performance analysis reports