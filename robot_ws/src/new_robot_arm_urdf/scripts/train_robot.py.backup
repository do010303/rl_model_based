#!/usr/bin/env python3

"""
DDPG Training Script for 4DOF Robot - Updated with Backup DDPG Agent
Uses the verified 3.5s action timing from test_simple_movement.py
"""

import sys
import os
import argparse
import time
import numpy as np

# Add paths for imports
sys.path.append('/home/ducanh/rl_model_based/robot_ws/src/new_robot_arm_urdf/scripts')
sys.path.append('/home/ducanh/rl_model_based')  # For backup DDPG agent

import rospy
from main_rl_environment_noetic import RLEnvironmentNoetic
from agents.ddpg_gazebo import DDPGAgentGazebo  # Backup DDPG agent (TensorFlow)

def train_robot(num_episodes=500, max_steps_per_episode=5, goal_tolerance=0.02):
    """
    Train the robot with DDPG agent from backup project
    
    Args:
        num_episodes: Number of training episodes (default: 500)
        max_steps_per_episode: Steps per episode (default: 5 for fast training)
        goal_tolerance: Distance threshold for goal achievement (default: 0.02m = 2cm)
    """
    print(f"ğŸš€ Starting DDPG Training for 4DOF Robot")
    print(f"ğŸ“Š Episodes: {num_episodes}")
    print(f"ğŸ“ Steps per episode: {max_steps_per_episode} (~{max_steps_per_episode * 3.5:.1f}s)")
    print(f"ğŸ¯ Goal tolerance: {goal_tolerance}m ({goal_tolerance*100:.1f}cm)")
    print("=" * 70)
    
    try:
        # Initialize ROS node
        rospy.init_node('robot_ddpg_trainer', anonymous=True)
        print("âœ“ ROS node initialized")
        
        # Create environment
        env = RLEnvironmentNoetic(
            max_episode_steps=max_steps_per_episode,
            goal_tolerance=goal_tolerance
        )
        print("âœ“ Environment created")
        
        # Wait for initialization
        print("Waiting for environment to initialize...")
        time.sleep(3)
        
        # Create DDPG agent (from backup - TensorFlow version)
        state_dim = 10  # [3 ee_pos + 4 joints + 3 target_pos]
        action_dim = 4  # 4 joint positions
        
        agent = DDPGAgentGazebo(state_dim=state_dim, n_actions=action_dim)
        print("âœ“ DDPG agent created")
        
        # Joint limits for denormalization
        joint_limits_low = np.array([-np.pi, -np.pi/2, -np.pi/2, -np.pi/2])
        joint_limits_high = np.array([np.pi, np.pi/2, np.pi/2, np.pi/2])
        
        # Training statistics
        successful_episodes = 0
        episode_rewards = []
        episode_distances = []
        
        print("=" * 70)
        print("ğŸ¯ Starting Training Loop")
        print("=" * 70)
        
        # Training loop
        for episode in range(num_episodes):
            episode_start = time.time()
            
            print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
            
            # Reset environment
            reset_success = env.reset_environment()
            if not reset_success:
                print(f"   âŒ Failed to reset environment")
                continue
            
            # Wait for reset to complete
            time.sleep(1.0)
                
            observation = env.get_state()
            if observation is None:
                print(f"   âŒ Failed to get initial state")
                continue
            
            episode_reward = 0
            min_distance = float('inf')
            episode_success = False
            
            # Episode loop
            for step in range(max_steps_per_episode):
                # Get action from DDPG agent (normalized [-1, 1])
                evaluate_mode = (episode % 10 == 0)  # Deterministic every 10 episodes
                action = agent.choose_action(observation, evaluate=evaluate_mode)
                
                # Denormalize action to joint positions (radians)
                joint_positions = joint_limits_low + (action + 1) * 0.5 * (joint_limits_high - joint_limits_low)
                joint_positions = np.clip(joint_positions, joint_limits_low, joint_limits_high)
                
                # Execute action (uses verified 3.5s timing from test_simple_movement.py)
                result = env.move_to_joint_positions(joint_positions)
                time.sleep(3.5)  # 3s trajectory + 0.5s buffer
                
                # Get next state and reward
                next_observation = env.get_state()
                if next_observation is None:
                    print(f"   âš ï¸ Failed to get state at step {step+1}")
                    break
                
                reward, done = env.calculate_reward()
                distance = env.get_distance_to_goal()
                
                # Store experience in replay buffer
                agent.remember(observation, action, reward, next_observation, done)
                
                # Update for next step
                observation = next_observation
                episode_reward += reward
                min_distance = min(min_distance, distance)
                
                # Log step
                print(f"   Step {step+1}/{max_steps_per_episode}: distance={distance:.4f}m, reward={reward:.2f}")
                
                # Check if goal achieved
                if done:
                    if reward > 0:
                        episode_success = True
                        successful_episodes += 1
                        print(f"   ğŸ† GOAL ACHIEVED!")
                    break
            
            # Learn from experience (multiple gradient updates per episode)
            actor_loss_ep = None
            critic_loss_ep = None
            
            if episode >= 50:  # Start learning after 50 episodes of exploration
                for _ in range(40):  # 40 gradient updates per episode
                    actor_loss, critic_loss = agent.learn()
                    if actor_loss is not None:
                        actor_loss_ep = actor_loss
                        critic_loss_ep = critic_loss
            
            # Episode summary
            episode_time = time.time() - episode_start
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
            
            # Calculate running averages
            avg_reward = np.mean(episode_rewards[-100:])
            success_rate = (successful_episodes / (episode + 1)) * 100
            
            print(f"\n   ğŸ“Š Episode Summary:")
            print(f"      Total reward: {episode_reward:.2f}")
            print(f"      Min distance: {min_distance:.4f}m ({min_distance*100:.2f}cm)")
            print(f"      Success: {'âœ… YES' if episode_success else 'âŒ NO'}")
            print(f"      Episode time: {episode_time:.1f}s")
            print(f"      Avg reward (100): {avg_reward:.2f}")
            print(f"      Success rate: {success_rate:.1f}%")
            
            if actor_loss_ep is not None:
                print(f"      Actor loss: {actor_loss_ep:.4f}")
                print(f"      Critic loss: {critic_loss_ep:.4f}")
            
            # Save checkpoint every 25 episodes
            if (episode + 1) % 25 == 0:
                agent.save_models(episode=episode+1)
                print(f"   ğŸ’¾ Checkpoint saved at episode {episode+1}")
        
        # Training complete
        print("\n" + "=" * 70)
        print("ğŸ‰ Training Complete!")
        print(f"   Total episodes: {num_episodes}")
        print(f"   Successful episodes: {successful_episodes}")
        print(f"   Success rate: {(successful_episodes/num_episodes)*100:.1f}%")
        print(f"   Final avg reward: {np.mean(episode_rewards[-100:]):.2f}")
        print("=" * 70)
        
        # Save final model
        agent.save_models()
        print("ğŸ’¾ Final model saved!")
        
        return episode_rewards, episode_distances
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user")
        if 'agent' in locals():
            agent.save_models()
            print("ğŸ’¾ Model saved before exit")
        
    except Exception as e:
        print(f"\nâŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
                
                observation = new_observation
                time.sleep(0.1)  # Small delay for visualization
            
            # Episode summary
            total_rewards.append(episode_reward)
            episode_lengths.append(step_count)
            
            print(f"Episode {episode + 1} Summary:")
            print(f"  âœ… Success: {episode_success}")
            print(f"  ğŸ“ Steps: {step_count}")
            print(f"  ğŸ Reward: {episode_reward:.2f}")
            print(f"  ğŸ¯ Success Rate: {successful_episodes}/{episode + 1} ({100*successful_episodes/(episode+1):.1f}%)")
        
        # Final training summary
        print("\n" + "=" * 50)
        print("ğŸ TRAINING COMPLETED!")
        print("=" * 50)
        print(f"ğŸ“Š Total Episodes: {num_episodes}")
        print(f"ğŸ† Successful Episodes: {successful_episodes}")
        print(f"ğŸ“ˆ Success Rate: {100*successful_episodes/num_episodes:.1f}%")
        print(f"ğŸ Average Reward: {np.mean(total_rewards):.2f}")
        print(f"ğŸ“ Average Episode Length: {np.mean(episode_lengths):.1f} steps")
        
        return True
        
    except Exception as e:
        print(f"âŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(description='Train 4DOF Robot for Drawing Tasks')
    parser.add_argument('--episodes', type=int, default=10, 
                        help='Number of training episodes (default: 10)')
    parser.add_argument('--max-steps', type=int, default=200,
                        help='Maximum steps per episode (default: 200)')
    parser.add_argument('--goal-tolerance', type=float, default=0.02,
                        help='Goal distance tolerance in meters (default: 0.02)')
    
    args = parser.parse_args()
    
    success = train_robot(
        num_episodes=args.episodes,
        max_steps_per_episode=args.max_steps,
        goal_tolerance=args.goal_tolerance
    )
    
    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()